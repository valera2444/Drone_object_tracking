{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab321f9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ab321f9",
    "outputId": "cad6c221-a3a2-4fc3-c8e3-0398a788e06e"
   },
   "outputs": [],
   "source": [
    "#Failed to build scikit-image lap\n",
    "#!pip install -r sort_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2onArRGOO4vP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2onArRGOO4vP",
    "outputId": "678551d5-264a-486f-9e9d-97be8af73f28"
   },
   "outputs": [],
   "source": [
    "#!pip install -r utils_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrJJyRkfPcp2",
   "metadata": {
    "id": "wrJJyRkfPcp2"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "STPAqNGn9LaH",
   "metadata": {
    "id": "STPAqNGn9LaH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from utils import write_metrics_to_df, write_time_to_df, replace_cuda_with_cpu, make_all_numpy,\\\n",
    " video_annot_txt_to_dataframe, motMetricsEnhancedCalculator, convert_array, usingPIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SZcAeBsPBvR5",
   "metadata": {
    "id": "SZcAeBsPBvR5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95cdb40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f0cd25",
   "metadata": {
    "id": "80f0cd25"
   },
   "outputs": [],
   "source": [
    "#path to sequeuence on local : D:\\Drone_object_tracking\\VisDrone2019-MOT-val\\sequences\\uav0000086_00000_v\n",
    "#path to sequeuence on colab /content/drive/MyDrive/VisDrone2019-MOT-val/sequences/uav0000086_00000_v\n",
    "from os import walk\n",
    "#uav0000086_00000_v\n",
    "#uav0000268_05773_v\n",
    "video_name = 'uav0000086_00000_v'\n",
    "dir_path_source_sequences = 'D:\\\\Drone_object_tracking\\\\VisDrone2019-MOT-val\\\\sequences'\n",
    "dir_path_source_annotations = 'D:\\\\Drone_object_tracking\\\\VisDrone2019-MOT-val\\\\annotations'\n",
    "dir_path = 'D:\\\\Drone_object_tracking\\\\VisDrone2019-MOT-val\\\\sequences'+video_name\n",
    "annot_path = 'D:\\\\Drone_object_tracking\\\\VisDrone2019-MOT-val\\\\annotations'+video_name+'.txt'\n",
    "\n",
    "\n",
    "#filenames = next(walk(dir_path), (None, None, []))[2]  # [] if no file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9rkVH0MAYX2v",
   "metadata": {
    "id": "9rkVH0MAYX2v"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3699fbd",
   "metadata": {
    "id": "c3699fbd"
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "#import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import cv2\n",
    "from time import time\n",
    "def make_predictions(model=None, tracker=None, dir_path_sequences=None, img_size=None):\n",
    "    '''\n",
    "    input:\n",
    "    model - model for predictions. MUST return predictions in formst like YOLOv8\n",
    "    tracker - MOT technic. SORT or DeepSORT\n",
    "    dir_path_sequences - string path to video sequence('D:\\\\Drone_object_tracking\\\\VisDrone2019-MOT-val\\\\sequences\\\\uav0000268_05773_v') as example\n",
    "    img_size - image size for YOLO inference\n",
    "\n",
    "    return:\n",
    "    preds - dataframe of predictions in MOT16 format\n",
    "    str_res - string with description of time characteristics of alghorithm\n",
    "    times - dataframe of time wasted for different parts of function\n",
    "    '''\n",
    "    #print(next(walk(dir_path_sequences)))\n",
    "    #print(dir_path_sequences+'/'+next(walk(dir_path_sequences)))\n",
    "    height, width = cv2.imread(dir_path_sequences+'/'+next(walk(dir_path_sequences))[2][0]).shape[:2]\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc('M', 'J', 'P', 'G')\n",
    "\n",
    "    try:\n",
    "        os.remove(\"video.avi\")\n",
    "    except: pass\n",
    "\n",
    "    video = cv2.VideoWriter(f'video{os.path.basename(dir_path_sequences)}.avi', fourcc, 20, (width, height))\n",
    "\n",
    "    mot_tracker = tracker\n",
    "\n",
    "    #acc = mm.MOTAccumulator(auto_id=True)\n",
    "    #annots = video_annot_txt_to_dataframe('D:\\\\Drone_object_tracking\\\\VisDrone2019-MOT-val\\\\annotations\\\\uav0000086_00000_v.txt')\n",
    "    columns=['frame_number','obj_id','left_top_x','left_top_y','width','height','confidence', 'category','truncation',\n",
    "                'occlusion']\n",
    "\n",
    "    preds = pd.DataFrame(columns=columns)\n",
    "    #print(len(filenames))\n",
    "\n",
    "    avg_yolo_time = 0.\n",
    "    avg_predictor_time = 0.\n",
    "    avg_frame_time_arr= []\n",
    "    avg_yolo_time_arr= []\n",
    "    avg_predictor_time = []\n",
    "    avg_resizing_time = []\n",
    "    avg_drawing_time = []\n",
    "    avg_reading_time = []\n",
    "\n",
    "    filenames = sorted(next(walk(dir_path_sequences), (None, None, []))[2])#BUG may appear\n",
    "    #print(filenames)\n",
    "    #i = 0\n",
    "    for idx, img_relative_path in enumerate(filenames):\n",
    "\n",
    "        time_before_fps  = time()\n",
    "        path = dir_path_sequences+'/'+img_relative_path\n",
    "\n",
    "        time_before_reading = time()\n",
    "        #img = cv2.imread(path)    #Using this, time after - before is NEARLY equal for 928px, for 640 still different(at the begging of small inference time is 20 ms)\n",
    "        img = usingPIL(path)\n",
    "        time_after_reading = time()\n",
    "        avg_reading_time.append((time_after_reading-time_before_reading)*1000)\n",
    "\n",
    "        time_before_resizing = time()\n",
    "\n",
    "        time_after_resizing = time()\n",
    "        avg_resizing_time.append((time_after_resizing-time_before_resizing)*1000)\n",
    "\n",
    "        time_before_YOLO = time()\n",
    "        prediction = model(img, verbose=False,imgsz=img_size)\n",
    "        time_after_YOLO = time()\n",
    "        boxes =  prediction[0].boxes.xyxy.type(torch.IntTensor).to('cuda')\n",
    "        scores = prediction[0].boxes.conf\n",
    "        classes = prediction[0].boxes.cls.type(torch.IntTensor)\n",
    "        scores = torch.unsqueeze(scores, 1)\n",
    "\n",
    "        torch_detections = torch.cat((boxes,scores), dim=1)\n",
    "        torch_detections_cpu = replace_cuda_with_cpu(torch_detections)\n",
    "        numpy_detections = make_all_numpy(torch_detections_cpu)\n",
    "\n",
    "        time_before_updating_tracker = time()\n",
    "        if type(mot_tracker) == DeepSort:\n",
    "            numpy_detections = np.concatenate((numpy_detections,np.expand_dims(classes.cpu().numpy(),axis=1) ),\n",
    "                                            axis=1)\n",
    "          #print('a')\n",
    "            result_tracker = mot_tracker.update_tracks(convert_array(numpy_detections), frame=img) # WARNING here\n",
    "          #print('b')\n",
    "            results = []\n",
    "\n",
    "            for res in result_tracker:\n",
    "                arr = res.to_tlwh().tolist()\n",
    "                arr.append(res.track_id)\n",
    "                results.append(arr)\n",
    "\n",
    "\n",
    "\n",
    "        else:\n",
    "            result_tracker = mot_tracker.update(numpy_detections)\n",
    "            results = result_tracker\n",
    "\n",
    "        time_after_updating_tracker = time()\n",
    "\n",
    "        time_before_drawing_predictions = time()\n",
    "\n",
    "        for res in results:\n",
    "\n",
    "            x1, y1, x2, y2, obj_id = [int(a) for a  in res]\n",
    "\n",
    "\n",
    "            cv2.rectangle(img, (x1, y1),(x2, y2), color=(255,0,0), thickness=2)\n",
    "            cv2.putText(img,str(obj_id), (x1, y1), cv2.FONT_HERSHEY_SIMPLEX,1, (0,255,0), thickness=2 )\n",
    "\n",
    "            single_pred = [idx+1,obj_id,x1, y1, x2-x1, y2-y1,1,1,1,1]\n",
    "            s = pd.Series(single_pred, index=columns)\n",
    "            preds = pd.concat([s.to_frame().T, preds], ignore_index=True, axis=0)\n",
    "\n",
    "        time_after_drawing_predictions = time()\n",
    "        video.write(img)  #about 30 seconds for uav0000086_00000_v\n",
    "        time_after_fps = time()\n",
    "\n",
    "        avg_drawing_time.append((time_after_drawing_predictions-time_before_drawing_predictions)*1000)\n",
    "        avg_frame_time_arr.append((time_after_fps-time_before_fps)*1000)\n",
    "        avg_yolo_time_arr.append(time_after_YOLO - time_before_YOLO)\n",
    "        avg_predictor_time.append(time_after_updating_tracker - time_before_updating_tracker)\n",
    "        #img = image_resize(img, width=width)\n",
    "\n",
    "\n",
    "\n",
    "    str_res = ''\n",
    "    str_res+=dir_path+'\\n'\n",
    "    str_res+= f'Resized to {img_size}\\n'\n",
    "    str_res+=f'image width:{width}, image height:{height}\\n'\n",
    "    str_res+=f'Image reading time ms: {sum(avg_reading_time)/len(avg_reading_time)}\\n'\n",
    "    str_res+=f'Image resizing time ms: {sum(avg_resizing_time)/len(avg_resizing_time)}\\n'\n",
    "    str_res+=f'Avarage Yolo time ms:{sum(avg_yolo_time_arr)/len(avg_yolo_time_arr) * 1000}\\n'\n",
    "    str_res+=f'Avarage predictor time ms:{sum(avg_predictor_time)/len(avg_predictor_time) * 1000}\\n'\n",
    "    str_res+=f'Drawing predictions time ms: {sum(avg_drawing_time)/len(avg_drawing_time)}\\n'\n",
    "\n",
    "    #print(len(preds))\n",
    "    times = write_time_to_df(video_name=dir_path_sequences,\n",
    "                             resized_to=img_size,\n",
    "                             source_width=width,\n",
    "                             source_height=height,\n",
    "                             avg_reading_time=sum(avg_reading_time)/len(avg_reading_time),\n",
    "                             avg_resizing_time=sum(avg_resizing_time)/len(avg_resizing_time),\n",
    "                             avg_yolo_time=sum(avg_yolo_time_arr)/len(avg_yolo_time_arr) * 1000,\n",
    "                             avg_predictor_time=sum(avg_predictor_time)/len(avg_predictor_time) * 1000,\n",
    "                             avg_drawing_time=sum(avg_drawing_time)/len(avg_drawing_time),\n",
    "                             avg_fps_time=sum(avg_frame_time_arr)/len(avg_frame_time_arr))\n",
    "\n",
    "    return preds, str_res, times\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6343bb",
   "metadata": {
    "id": "ae6343bb"
   },
   "outputs": [],
   "source": [
    "def proccess_single_video(model=None, tracker=None, dir_path_source_sequences=None, dir_path_source_annotations=None,\n",
    "                          video_name=None, img_sz=None):\n",
    "    \"\"\"\n",
    "    proccesses one video: find time and accuracy metrics on given video\n",
    "\n",
    "    input:\n",
    "    dir_path_source_sequences - '/content/drive/MyDrive/VisDrone2019-MOT-val/sequences/'\n",
    "    dir_path_source_annotations - '/content/drive/MyDrive/VisDrone2019-MOT-val/annotations/'\n",
    "    video_name - name of video for both annots and sequences\n",
    "\n",
    "    return:\n",
    "    time_df - time results dataframe\n",
    "    metrics_df - metrics results dataframe\n",
    "    \"\"\"\n",
    "    print(video_name,img_sz)\n",
    "    preds, res_str, time_df = make_predictions(model,tracker, dir_path_source_sequences+'/'+video_name,img_sz)\n",
    "\n",
    "    annots = video_annot_txt_to_dataframe(f'{dir_path_source_annotations}/{video_name}.txt')\n",
    "\n",
    "    MOT, metrics_df = motMetricsEnhancedCalculator(annots.to_numpy(), preds.to_numpy() )\n",
    "\n",
    "    return time_df, metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dd0a5f",
   "metadata": {
    "id": "83dd0a5f"
   },
   "outputs": [],
   "source": [
    "def proccess_all_videos():\n",
    "    time_columns=['video_name','resized_to', 'source_width','source_height','avg_reading_time','avg_resizing_time',\n",
    "              'avg_yolo_time','avg_predictor_time','avg_drawing_time']\n",
    "    times_dataframe = pd.DataFrame(columns=time_columns)\n",
    "\n",
    "\n",
    "    metric_columns = ['num_frames', 'idf1', 'idp', 'idr', 'recall', 'precision', 'num_objects',  'mostly_tracked',\n",
    "                    'partially_tracked', 'mostly_lost', 'num_false_positives', 'num_misses', 'num_switches',\n",
    "                    'num_fragmentations', 'mota', 'motp']\n",
    "    metrics_dataframe = pd.DataFrame(columns=metric_columns)\n",
    "\n",
    "    with open('results.txt', 'w') as f:\n",
    "        for video in  os.listdir(dir_path_source_sequences):\n",
    "            for s in [640]:#[640,928]\n",
    "\n",
    "                print(video)\n",
    "                print(s)\n",
    "\n",
    "                times_df, metrics_df = proccess_single_video(model, tracker,dir_path_source_sequences,\n",
    "                                                             dir_path_source_annotations,video, s)\n",
    "\n",
    "                times_dataframe = pd.concat([times_df, times_dataframe], ignore_index=True, axis=0)\n",
    "                metrics_dataframe = pd.concat([metrics_dataframe, metrics_df], ignore_index=True, axis=0)\n",
    "\n",
    "    return times_dataframe, metrics_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8jM-HzVP5CEB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8jM-HzVP5CEB",
    "outputId": "0bc60ff3-6a92-485d-addb-b9cf6e2fe3e6"
   },
   "outputs": [],
   "source": [
    "\n",
    "#!pip install ultralytics\n",
    "#https://github.com/levan92/deep_sort_realtime/blob/master/deep_sort_realtime/embedder/embedder_pytorch.py\n",
    "#!pip install deep-sort-realtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Au9uhUEq5eZ0",
   "metadata": {
    "id": "Au9uhUEq5eZ0"
   },
   "outputs": [],
   "source": [
    "from sort import Sort    # crashes kernel\n",
    "#from deepsort.tracker import DeepSortTracker\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PLG7OcEk76pK",
   "metadata": {
    "id": "PLG7OcEk76pK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AMoE5LR-47iU",
   "metadata": {
    "id": "AMoE5LR-47iU"
   },
   "outputs": [],
   "source": [
    "# Load a pretrained model\n",
    "# Weights from https://huggingface.co/ENOT-AutoDL/yolov8s_visdrone/tree/main/enot_neural_architecture_selection_x3/weights\n",
    "\n",
    "\n",
    "model = YOLO(\"YOLOv8s(x3).pt\")\n",
    "\n",
    "model.to('cuda');\n",
    "#tracker = DeepSort(max_age=20,\n",
    "#                   nms_max_overlap=0.7)\n",
    "tracker = Sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7dd17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df, metrics_df = proccess_all_videos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m8NYdhqb5z4f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "id": "m8NYdhqb5z4f",
    "outputId": "10d8b1c1-e867-4d2d-c2e1-2eb7a2812c96"
   },
   "outputs": [],
   "source": [
    "time_df, metrics_df = proccess_single_video(model=model,\n",
    "                                            tracker=tracker,\n",
    "                                            dir_path_source_sequences=dir_path_source_sequences,\n",
    "                                            dir_path_source_annotations=dir_path_source_annotations,\n",
    "                                            video_name=video_name,\n",
    "                                            img_sz=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "az_rccssjbX5",
   "metadata": {
    "id": "az_rccssjbX5"
   },
   "outputs": [],
   "source": [
    "time_df['clear_fps'] = time_df['avg_reading_time'] + time_df['avg_yolo_time'] + time_df['avg_predictor_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3784d4c",
   "metadata": {
    "id": "a3784d4c"
   },
   "outputs": [],
   "source": [
    "result = pd.concat((time_df, metrics_df), axis=1, ignore_index=False)\n",
    "result.to_csv('YOLOv8_s(x3)+DeepSORT.csv', sep=';', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DiveUIKq5HDz",
   "metadata": {
    "id": "DiveUIKq5HDz"
   },
   "outputs": [],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6ad1f2",
   "metadata": {
    "id": "2d6ad1f2"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98ac7b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "id": "a98ac7b2",
    "outputId": "e9eedcb6-1cf6-48b1-d8c3-2abbd768c282"
   },
   "outputs": [],
   "source": [
    "sns.heatmap(time_df.drop(['video_name'], axis=1).astype(np.float64).corr(), annot=True, annot_kws={\"size\": 5})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39985b60",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "39985b60",
    "outputId": "ac0b25a8-b1ce-4e53-e283-3506d40d9b6e"
   },
   "outputs": [],
   "source": [
    "result.sort_values('clear_fps')\\\n",
    " [['resized_to','source_width',\t'source_height','avg_reading_time','avg_yolo_time','avg_predictor_time','clear_fps']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc8dd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(dir_path_source_annotations+'\\\\'+video_name+'.txt')\n",
    "print(len(f.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a85c7ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9601278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_videos():\n",
    "\n",
    "    image_folder = dir_path_source_sequences\n",
    "    video_folder = '\\\\videos'\n",
    "    video_names = [img for img in os.listdir(image_folder)]\n",
    "    #width,height = 10,10\n",
    "\n",
    "    for name in video_names:\n",
    "        #print(next(os.walk(image_folder+'\\\\'+name))[2][0])\n",
    "        frame = cv2.imread(os.path.join(image_folder,name,next(os.walk(image_folder+'\\\\'+name))[2][0]))\n",
    "\n",
    "        height, width, layers = frame.shape\n",
    "        print(video_folder+'\\\\'+name)\n",
    "        video = cv2.VideoWriter(\"D:\\\\Drone_object_tracking\\\\\"+video_folder+'\\\\'+name+'.avi', 0, 24, (width,height))\n",
    "\n",
    "        for img in os.listdir(image_folder+'\\\\'+name):\n",
    "            video.write(cv2.imread(os.path.join(image_folder,name,img)))\n",
    "\n",
    "        #print(name)\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf86020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_videos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300d3cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall torchreid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25152fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beeb3195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a65b7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchreid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f980190",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The given name already exists, please choose another name excluding ['mars', 'ilidsvid', 'prid2011', 'dukemtmcvidreid', 'VisDrone2019MOT']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchreid\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mVisDrone2019MOT\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VisDrone2019MOT\n\u001b[1;32m----> 3\u001b[0m torchreid\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mregister_video_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVisDrone2019MOT\u001b[39m\u001b[38;5;124m'\u001b[39m, VisDrone2019MOT)\n",
      "File \u001b[1;32mD:\\Drone_object_tracking\\torchreid\\data\\datasets\\__init__.py:115\u001b[0m, in \u001b[0;36mregister_video_dataset\u001b[1;34m(name, dataset)\u001b[0m\n\u001b[0;32m    113\u001b[0m curr_datasets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(__video_datasets\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m curr_datasets:\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    116\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe given name already exists, please choose \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    117\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manother name excluding \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(curr_datasets)\n\u001b[0;32m    118\u001b[0m     )\n\u001b[0;32m    119\u001b[0m __video_datasets[name] \u001b[38;5;241m=\u001b[39m dataset\n",
      "\u001b[1;31mValueError\u001b[0m: The given name already exists, please choose another name excluding ['mars', 'ilidsvid', 'prid2011', 'dukemtmcvidreid', 'VisDrone2019MOT']"
     ]
    }
   ],
   "source": [
    "from torchreid.data.datasets.video.VisDrone2019MOT import VisDrone2019MOT\n",
    "\n",
    "torchreid.data.register_video_dataset('VisDrone2019MOT', VisDrone2019MOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adb83f34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building train transforms ...\n",
      "+ resize to 256x128\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "Building test transforms ...\n",
      "+ resize to 256x128\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "=> Loading train (source) dataset\n",
      "=> Loaded VisDrone2019MOT\n",
      "  -------------------------------------------\n",
      "  subset   | # ids | # tracklets | # cameras\n",
      "  -------------------------------------------\n",
      "  train    |    32 |          32 |         1\n",
      "  query    |    26 |          27 |         1\n",
      "  gallery  |    85 |         138 |         1\n",
      "  -------------------------------------------\n",
      "=> Loading test (target) dataset\n",
      "=> Loaded VisDrone2019MOT\n",
      "  -------------------------------------------\n",
      "  subset   | # ids | # tracklets | # cameras\n",
      "  -------------------------------------------\n",
      "  train    |    32 |          32 |         1\n",
      "  query    |    50 |          51 |         1\n",
      "  gallery  |   152 |         255 |         1\n",
      "  -------------------------------------------\n",
      "\n",
      "\n",
      "  **************** Summary ****************\n",
      "  source             : ['VisDrone2019MOT']\n",
      "  # source datasets  : 1\n",
      "  # source ids       : 32\n",
      "  # source tracklets : 32\n",
      "  # source cameras   : 1\n",
      "  target             : ['VisDrone2019MOT']\n",
      "  *****************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datamanager = torchreid.data.VideoDataManager(\n",
    "    root='D:\\\\Drone_object_tracking\\\\reid-data\\\\VisDrone2019-MOT-train\\\\uav0000013_01073_v',\n",
    "    sources='VisDrone2019MOT',\n",
    "    batch_size_train = 3,    #how many tracklets in a batch\n",
    "    batch_size_test =3,    #how many tracklets in a batch\n",
    "    num_cams = 1,\n",
    "    seq_len = 15,    # how many frmaes in a tracklet\n",
    "    num_instances = 4,    #how many tracklets per id in a batch\n",
    "    num_datasets = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23432d11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Start training\n",
      "epoch: [1/10][10/10]\ttime 1.546 (5.374)\tdata 0.000 (3.883)\teta 0:08:03\tloss_t 1.0016 (2.5101)\tlr 0.000300\n",
      "epoch: [2/10][10/10]\ttime 1.564 (3.098)\tdata 0.001 (1.636)\teta 0:04:07\tloss_t 0.9001 (1.8151)\tlr 0.000300\n",
      "epoch: [3/10][10/10]\ttime 1.532 (3.279)\tdata 0.000 (1.775)\teta 0:03:49\tloss_t 0.0841 (0.4746)\tlr 0.000300\n",
      "epoch: [4/10][10/10]\ttime 1.532 (2.987)\tdata 0.000 (1.525)\teta 0:02:59\tloss_t 0.0000 (0.1423)\tlr 0.000300\n",
      "epoch: [5/10][10/10]\ttime 1.529 (3.153)\tdata 0.000 (1.660)\teta 0:02:37\tloss_t 1.0189 (0.3517)\tlr 0.000300\n",
      "epoch: [6/10][10/10]\ttime 1.651 (3.248)\tdata 0.000 (1.748)\teta 0:02:09\tloss_t 0.2032 (0.0866)\tlr 0.000300\n",
      "epoch: [7/10][10/10]\ttime 1.690 (3.272)\tdata 0.000 (1.764)\teta 0:01:38\tloss_t 0.0000 (0.0656)\tlr 0.000300\n",
      "epoch: [8/10][10/10]\ttime 1.547 (3.246)\tdata 0.000 (1.787)\teta 0:01:04\tloss_t 0.0013 (0.4456)\tlr 0.000300\n",
      "epoch: [9/10][10/10]\ttime 1.583 (3.113)\tdata 0.000 (1.640)\teta 0:00:31\tloss_t 0.0013 (0.2620)\tlr 0.000300\n",
      "epoch: [10/10][10/10]\ttime 1.543 (3.379)\tdata 0.000 (1.877)\teta 0:00:00\tloss_t 0.5170 (0.0916)\tlr 0.000300\n",
      "=> Final test\n",
      "##### Evaluating VisDrone2019MOT (source) #####\n",
      "Extracting features from query set ...\n",
      "Done, obtained 51-by-2048 matrix\n",
      "Extracting features from gallery set ...\n",
      "Done, obtained 138-by-2048 matrix\n",
      "Speed: 0.0105 sec/batch\n",
      "Computing distance matrix with metric=euclidean ...\n",
      "Computing CMC and mAP ...\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Error: all query identities do not appear in gallery",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 20\u001b[0m\n\u001b[0;32m     10\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torchreid\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mbuild_lr_scheduler(\n\u001b[0;32m     11\u001b[0m     optimizer,\n\u001b[0;32m     12\u001b[0m     lr_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msingle_step\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     13\u001b[0m     stepsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m engine \u001b[38;5;241m=\u001b[39m torchreid\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39mVideoTripletEngine(\n\u001b[0;32m     16\u001b[0m     datamanager, model, optimizer, margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m,\n\u001b[0;32m     17\u001b[0m     weight_t\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, weight_x\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, scheduler\u001b[38;5;241m=\u001b[39mscheduler,\n\u001b[0;32m     18\u001b[0m     pooling_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     19\u001b[0m )\n\u001b[1;32m---> 20\u001b[0m engine\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m     21\u001b[0m     max_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     22\u001b[0m     save_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog/resnet50-triplet-mars\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     23\u001b[0m     print_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m#if (self.batch_idx + 1) % print_freq == 0:, where for self.batch_idx, data in enumerate(self.train_loader):\u001b[39;00m\n\u001b[0;32m     24\u001b[0m )\n",
      "File \u001b[1;32mD:\\Drone_object_tracking\\torchreid\\engine\\engine.py:213\u001b[0m, in \u001b[0;36mEngine.run\u001b[1;34m(self, save_dir, max_epoch, start_epoch, print_freq, fixbase_epoch, open_layers, start_eval, eval_freq, test_only, dist_metric, normalize_feature, visrank, visrank_topk, use_metric_cuhk03, ranks, rerank)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_epoch \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=> Final test\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 213\u001b[0m     rank1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest(\n\u001b[0;32m    214\u001b[0m         dist_metric\u001b[38;5;241m=\u001b[39mdist_metric,\n\u001b[0;32m    215\u001b[0m         normalize_feature\u001b[38;5;241m=\u001b[39mnormalize_feature,\n\u001b[0;32m    216\u001b[0m         visrank\u001b[38;5;241m=\u001b[39mvisrank,\n\u001b[0;32m    217\u001b[0m         visrank_topk\u001b[38;5;241m=\u001b[39mvisrank_topk,\n\u001b[0;32m    218\u001b[0m         save_dir\u001b[38;5;241m=\u001b[39msave_dir,\n\u001b[0;32m    219\u001b[0m         use_metric_cuhk03\u001b[38;5;241m=\u001b[39muse_metric_cuhk03,\n\u001b[0;32m    220\u001b[0m         ranks\u001b[38;5;241m=\u001b[39mranks\n\u001b[0;32m    221\u001b[0m     )\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch, rank1, save_dir)\n\u001b[0;32m    224\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m time_start)\n",
      "File \u001b[1;32mD:\\Drone_object_tracking\\torchreid\\engine\\engine.py:325\u001b[0m, in \u001b[0;36mEngine.test\u001b[1;34m(self, dist_metric, normalize_feature, visrank, visrank_topk, save_dir, use_metric_cuhk03, ranks, rerank)\u001b[0m\n\u001b[0;32m    323\u001b[0m query_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_loader[name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    324\u001b[0m gallery_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_loader[name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgallery\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 325\u001b[0m rank1, mAP \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate(\n\u001b[0;32m    326\u001b[0m     dataset_name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m    327\u001b[0m     query_loader\u001b[38;5;241m=\u001b[39mquery_loader,\n\u001b[0;32m    328\u001b[0m     gallery_loader\u001b[38;5;241m=\u001b[39mgallery_loader,\n\u001b[0;32m    329\u001b[0m     dist_metric\u001b[38;5;241m=\u001b[39mdist_metric,\n\u001b[0;32m    330\u001b[0m     normalize_feature\u001b[38;5;241m=\u001b[39mnormalize_feature,\n\u001b[0;32m    331\u001b[0m     visrank\u001b[38;5;241m=\u001b[39mvisrank,\n\u001b[0;32m    332\u001b[0m     visrank_topk\u001b[38;5;241m=\u001b[39mvisrank_topk,\n\u001b[0;32m    333\u001b[0m     save_dir\u001b[38;5;241m=\u001b[39msave_dir,\n\u001b[0;32m    334\u001b[0m     use_metric_cuhk03\u001b[38;5;241m=\u001b[39muse_metric_cuhk03,\n\u001b[0;32m    335\u001b[0m     ranks\u001b[38;5;241m=\u001b[39mranks,\n\u001b[0;32m    336\u001b[0m     rerank\u001b[38;5;241m=\u001b[39mrerank\n\u001b[0;32m    337\u001b[0m )\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/rank1\u001b[39m\u001b[38;5;124m'\u001b[39m, rank1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch)\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Drone_object_tracking\\torchreid\\engine\\engine.py:408\u001b[0m, in \u001b[0;36mEngine._evaluate\u001b[1;34m(self, dataset_name, query_loader, gallery_loader, dist_metric, normalize_feature, visrank, visrank_topk, save_dir, use_metric_cuhk03, ranks, rerank)\u001b[0m\n\u001b[0;32m    405\u001b[0m     distmat \u001b[38;5;241m=\u001b[39m re_ranking(distmat, distmat_qq, distmat_gg)\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComputing CMC and mAP ...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 408\u001b[0m cmc, mAP \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39mevaluate_rank(\n\u001b[0;32m    409\u001b[0m     distmat,\n\u001b[0;32m    410\u001b[0m     q_pids,\n\u001b[0;32m    411\u001b[0m     g_pids,\n\u001b[0;32m    412\u001b[0m     q_camids,\n\u001b[0;32m    413\u001b[0m     g_camids,\n\u001b[0;32m    414\u001b[0m     use_metric_cuhk03\u001b[38;5;241m=\u001b[39muse_metric_cuhk03\n\u001b[0;32m    415\u001b[0m )\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m** Results **\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmAP: \u001b[39m\u001b[38;5;132;01m{:.1%}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(mAP))\n",
      "File \u001b[1;32mD:\\Drone_object_tracking\\torchreid\\metrics\\rank.py:204\u001b[0m, in \u001b[0;36mevaluate_rank\u001b[1;34m(distmat, q_pids, g_pids, q_camids, g_camids, max_rank, use_metric_cuhk03, use_cython)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m evaluate_cy(\n\u001b[0;32m    200\u001b[0m         distmat, q_pids, g_pids, q_camids, g_camids, max_rank,\n\u001b[0;32m    201\u001b[0m         use_metric_cuhk03\n\u001b[0;32m    202\u001b[0m     )\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m evaluate_py(\n\u001b[0;32m    205\u001b[0m         distmat, q_pids, g_pids, q_camids, g_camids, max_rank,\n\u001b[0;32m    206\u001b[0m         use_metric_cuhk03\n\u001b[0;32m    207\u001b[0m     )\n",
      "File \u001b[1;32mD:\\Drone_object_tracking\\torchreid\\metrics\\rank.py:164\u001b[0m, in \u001b[0;36mevaluate_py\u001b[1;34m(distmat, q_pids, g_pids, q_camids, g_camids, max_rank, use_metric_cuhk03)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m eval_cuhk03(\n\u001b[0;32m    161\u001b[0m         distmat, q_pids, g_pids, q_camids, g_camids, max_rank\n\u001b[0;32m    162\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m eval_market1501(\n\u001b[0;32m    165\u001b[0m         distmat, q_pids, g_pids, q_camids, g_camids, max_rank\n\u001b[0;32m    166\u001b[0m     )\n",
      "File \u001b[1;32mD:\\Drone_object_tracking\\torchreid\\metrics\\rank.py:147\u001b[0m, in \u001b[0;36meval_market1501\u001b[1;34m(distmat, q_pids, g_pids, q_camids, g_camids, max_rank)\u001b[0m\n\u001b[0;32m    144\u001b[0m     AP \u001b[38;5;241m=\u001b[39m tmp_cmc\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m num_rel\n\u001b[0;32m    145\u001b[0m     all_AP\u001b[38;5;241m.\u001b[39mappend(AP)\n\u001b[1;32m--> 147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m num_valid_q \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError: all query identities do not appear in gallery\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    149\u001b[0m all_cmc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(all_cmc)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    150\u001b[0m all_cmc \u001b[38;5;241m=\u001b[39m all_cmc\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m/\u001b[39m num_valid_q\n",
      "\u001b[1;31mAssertionError\u001b[0m: Error: all query identities do not appear in gallery"
     ]
    }
   ],
   "source": [
    "model = torchreid.models.build_model(\n",
    "    name='resnet50',\n",
    "    num_classes=datamanager.num_train_pids,\n",
    "    loss='triplet'\n",
    ")\n",
    "model = model.cuda()\n",
    "optimizer = torchreid.optim.build_optimizer(\n",
    "    model, optim='adam', lr=0.0003\n",
    ")\n",
    "scheduler = torchreid.optim.build_lr_scheduler(\n",
    "    optimizer,\n",
    "    lr_scheduler='single_step',\n",
    "    stepsize=20\n",
    ")\n",
    "engine = torchreid.engine.VideoTripletEngine(\n",
    "    datamanager, model, optimizer, margin=0.3,\n",
    "    weight_t=1, weight_x=0, scheduler=scheduler,\n",
    "    pooling_method='avg'\n",
    ")\n",
    "engine.run(\n",
    "    max_epoch=10,\n",
    "    save_dir='log/resnet50-triplet-mars',\n",
    "    print_freq=10  #if (self.batch_idx + 1) % print_freq == 0:, where for self.batch_idx, data in enumerate(self.train_loader):\n",
    ")\n",
    "#loss_t - triplet loss\n",
    "#loss_x - cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b391a2dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2c445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_train_root_dir = 'D:\\\\Drone_object_tracking\\\\VisDrone2019-MOT-train\\\\'\n",
    "video_val_root_dir = 'D:\\\\Drone_object_tracking\\\\VisDrone2019-MOT-val\\\\'\n",
    "\n",
    "sequences_dir = 'sequences\\\\'\n",
    "annot_dir = 'annotations\\\\'\n",
    "\n",
    "save_root_train_dir = 'D:\\\\Drone_object_tracking\\\\reid-data\\\\VisDrone2019-MOT-train\\\\'\n",
    "save_root_val_dir = 'D:\\\\Drone_object_tracking\\\\reid-data\\\\VisDrone2019-MOT-val\\\\'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6907a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b638287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import video_annot_txt_to_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4f3097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f64d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SAVE_DFS = dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baea0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caa9ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_VisDrone_to_ReID()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d50fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c3dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visdrone_to_df(video_root_dir, video_annot_dir, video_seq_dir):\n",
    "    \"\"\"\n",
    "    converts visdrone from txt to dataframes. Each video - one dataframe\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    dirs = os.listdir(osp.join(video_root_dir,video_seq_dir))\n",
    "    \n",
    "    for name in dirs:\n",
    "        dfs.append(video_annot_txt_to_dataframe(osp.join(video_root_dir,video_annot_dir,name+'.txt')))\n",
    "        print(f'{name} converted to df')\n",
    "    return dfs\n",
    "#dfs.append(video_annot_txt_to_dataframe(osp.join(video_train_root_dir,annot_train_dir,name+'.txt')))\n",
    "#dirs = os.listdir(osp.join(video_root_dir,sequences_train_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca44473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_from_df(dfs,video_root_dir,video_annot_dir, video_seq_dir, save_dir ):\n",
    "    \"\"\"\n",
    "    write from dataframe to file system. Data will have next form:\n",
    "    -root\n",
    "    --video1\n",
    "    ---id1\n",
    "    ----img1\n",
    "    ----img2\n",
    "    ----....\n",
    "    ---id2\n",
    "    ----img1\n",
    "    ----img2\n",
    "    --video2\n",
    "    ---id1\n",
    "    ----img1\n",
    "    ----img2\n",
    "    ---id2\n",
    "    ----img1\n",
    "    ----img2\n",
    "    ----....\n",
    "    \"\"\"\n",
    "    dirs = os.listdir(osp.join(video_root_dir,video_seq_dir))\n",
    "\n",
    "    for (name,df) in zip(dirs, dfs):#для 1го видео\n",
    "\n",
    "            frame_pathes = [osp.join(video_root_dir, video_seq_dir, name, f_n) \\\n",
    "                            for f_n in os.listdir(osp.join(video_root_dir, video_seq_dir,name))]\n",
    "\n",
    "            #print(frame_pathes)\n",
    "            for frame_id in pd.unique(df['frame_index']):\n",
    "                df_per_frame_id = df[df['frame_index'] == frame_id]\n",
    "\n",
    "                id_counter = 1    #ids unique per video\n",
    "\n",
    "                img = cv2.imread(frame_pathes[frame_id - 1])\n",
    "                #print(img)\n",
    "                for row_in_image in df_per_frame_id.iterrows():\n",
    "\n",
    "                    f_i,target_id,left_top_x,left_top_y,width,height,score,category,truncation,occlusion=list(row_in_image)[1]\n",
    "\n",
    "                    id_path = osp.join(save_dir,name,str(target_id))\n",
    "                    if not os.path.exists(id_path):\n",
    "                        os.makedirs(id_path)\n",
    "\n",
    "\n",
    "                    cv2.imwrite(osp.join(id_path,str(frame_id)+'.jpg'), img[left_top_y:left_top_y+height,\n",
    "                                                                            left_top_x:left_top_x+width])\n",
    "                    \n",
    "            print(f'{name} proccessed and written')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a228836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_train():\n",
    "    dfs = visdrone_to_df(video_train_root_dir,annot_dir,sequences_dir )\n",
    "    write_from_df(dfs,video_train_root_dir,annot_dir,sequences_dir,save_root_train_dir  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0930e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_validation():\n",
    "    dfs = visdrone_to_df(video_val_root_dir,annot_dir,sequences_dir )\n",
    "    write_from_df(dfs,video_val_root_dir,annot_dir,sequences_dir,save_root_val_dir  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e873e188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_dataset(root):\n",
    "    \"\"\"\n",
    "    delete id's with less than 10 frames\n",
    "    \"\"\"\n",
    "    deleted_ids=0\n",
    "    videos_dir = os.listdir(root)\n",
    "    dic={}\n",
    "    lst = []\n",
    "    for video in videos_dir:\n",
    "        video_dir = osp.join(root,video)\n",
    "        idxs_dir = os.listdir(video_dir)\n",
    "        for idx in idxs_dir:\n",
    "            idx_dir = osp.join(video_dir,idx)\n",
    "            #if len(os.listdir(idx_dir)) < 20:\n",
    "            #    if video not in dic:\n",
    "            #        dic[video]=1\n",
    "            #    else:\n",
    "            #        dic[video]+=1\n",
    "            #    deleted_ids+=1\n",
    "                #print(idx_dir)\n",
    "            lst.append(len(os.listdir(idx_dir)))\n",
    "                \n",
    "        print(video)\n",
    "        \n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b7c320",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root = 'D:\\\\Drone_object_tracking\\\\reid-data\\\\VisDrone2019-MOT-train'\n",
    "numbers_of_frames_per_id = clear_dataset(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13639062",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(numbers_of_frames_per_id).hist(bins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c02e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(numbers_of_frames_per_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e95dae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
